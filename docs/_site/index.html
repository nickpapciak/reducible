<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>CS-4641-Project</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="CS-4641-Project" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="CS-4641-Project" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="CS-4641-Project" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"CS-4641-Project","name":"CS-4641-Project","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="icon" type="image/png" href="assets/img/favicon.ico">
    <link rel="stylesheet" href="/assets/css/style.css?v=fa25bd9cb8e05f8fcc5eb41919edeb6890ff105a">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://mahdi-roozbahani.github.io/CS46417641-fall2023">CS 4641- Machine Learning</a></h1>

          <img src="assets/img/infographic.jpg" alt="infographic" />

        <p></p>

        
        <p class="view"><a href="https://github.com/nickpapciak/CS-4641-Project">View the Project on GitHub <small>nickpapciak/CS-4641-Project</small></a></p>
        

        

        
      </header>
      <section>

      <h1 align="center">Using Computer Vision to Translate American Sign Language </h1>

<h2 id="motivation">Motivation</h2>
<p>Over half a million people worldwide use American Sign Language (ASL) to communicate. For these people, being able to communicate quickly and transparently is a luxury, not an everyday occurrence. Training a computer to read ASL could be an accurate and cost-effective way to lower the barrier of communication between hearing and non-hearing people.</p>

<h2 id="how-do-we-solve-this">How Do We Solve This?</h2>
<p>General sign language detection is hard and lacks the enormous quantity of data that most language models require. Therefore, we are focusing on only the 26 Latin letters (with punctuation and digits). This makes our effort simpler, more cost effective, and the techniques we use may one day be expanded upon to build a more general model.</p>

<p>There are two standards of ASL comprehension. The first is <em>individual character comprehension</em> where single images are translated to their corresponding ASL character. <a href="https://ieeexplore.ieee.org/abstract/document/8756576">Köpüklü et al.</a> demonstrate that similar recognition tasks can be performed with high (&gt;90%) accuracy using convolutional neural networks (CNNs). The second, more general comprehension is <em>translation</em>. When experienced fingerspellers “speak”, they coarticulate their letters, changing the appearance of future letters. This makes translation a sequence-to-sequence problem, where not only individual signs but the greater context and grammar are relevant. Previous approaches like <a href="https://www.sign-lang.uni-hamburg.de/lrec/pub/18005.html">Metaxas et al.</a> used a CRF model to solve the sequence-to-sequence problem. We, however, are going to be using the <a href="https://arxiv.org/abs/1706.03762">Transformer</a> architecture, an architecture motivated by language translation, which makes it well-suited for this challenge.</p>

<h2 id="data">Data</h2>
<p>To train for individual recognition, we were initially planning on using an MNIST-like <a href="https://www.kaggle.com/datasets/datamunge/sign-language-mnist">dataset</a> 
of the ASL alphabet. However, we found that the quality of images was too low for proper training of our model, so we instead switch to one with higher quality <a href="https://www.kaggle.com/datasets/danrasband/asl-alphabet-test/data">dataset</a>. We could use this data to train a CNN, but video data is high-dimensional. Previous works <a href="https://www.semanticscholar.org/paper/Human-Posture-Recognition-in-Video-Sequence-Boulay-Antipolis/87ebfff1aa37f60e47b9b12c8fea32da331064b1">(Boulay et al.)</a> on detection used PCA for dimensionality reduction, but we will instead convert the images to MediaPipe<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> landmarks.</p>

<p>For translation, we found a Google sponsored <a href="https://www.kaggle.com/competitions/asl-fingerspelling/overview">Kaggle competition</a> with hundreds of gigabytes of data. It consists of processed videos, where each frame is not an image but rather a collection of MediaPipe landmarks and their positions, along with the English phrase being signed.</p>

<p><img src="assets/img/mediapipe_labels.png?raw=true" alt="MediaPipe Landmarks" title="Landmarks" /></p>
<ol><li id="fn:1" role="doc-endnote"><p>
Google has many models under the MediaPipe umbrella, but we will be using their pre-trained hand detection model which detects up to 63 landmarks (i.e. features) of the human hand.&nbsp;
<a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p></li></ol>

<h2 id="methodology">Methodology</h2>
<p>Our pipeline goes as follows:</p>
<ol>
  <li>We take our data and conduct pre-processing through MediaPipe Landmarks as discussed previously. In doing so, we will be able to take each photo (which each constitutes thousands of pixels) into 63 particular features – these are denoted as “hand landmarks”, specific points along the hand which the model will use to predict ASL letters.</li>
  <li>Once our dataset is cleaned and pre-processed, we implemented our supervised machine learning algorithm – K-nearest Neighbours (also known as KNN). This is a classification algorithm which utilizes proximity as a measurement from other data points in order to determine which group each data point most likely belongs to. We also use a Gaussian Mixture model in a semi-supervised way (by initializing cluster means with our correct labels).</li>
  <li>In doing so, we will be able to precisely measure and our predictive model’s performance based on several criteria: namely, with the use of a confusion matrix to predict the relationship between the predicted and actual results, as well as scores for accuracy, precision, recall, and f1. The following section, titled “Results”, details and visualizes what we achieved.</li>
</ol>

<h2 id="what-can-we-do-with-this">What Can We Do With This?</h2>
<p>A well-trained model could be used for a learning platform to teach fingerspelling, where a webcam monitors students to track their accuracy. A translational model could be used in an app where, with a cellphone camera, an ASL native could easily communicate with a non-speaker.</p>

<h2 id="results">Results</h2>
<p>We first reduced our data to mediapipe landmarks</p>

<p><img src="assets/img/mediapipe_table.png" width="800" /></p>

<h3 id="k-nearest-neighbors-classifier">K-Nearest Neighbors Classifier</h3>
<p>Subsequently, as stated above we implemented the KNN algorithm, utilizing the <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors">sklearn</a> library to implement it. This library allowed us to create a classifier and fit it to our training data, in which it was then used to predict our testing data.</p>
<p style="font-family:'Courier New'">Accuracy: 0.658 <br /> Precision: 0.790 <br /> Recall: 0.658 <br /> F1 Score: 0.691</p>

<p><img src="assets/img/confusion_matrix.png" width="275" />
<img src="assets/img/roc.png" width="275" />
<img src="assets/img/prerec.png" width="275" /></p>

<p>The confusion matrix details how our model performed in terms of true positive/negatives, as well as false positive/negatives. As you can see, there is a clear distinction along the diagonal where the predicted results match the actual ones. The ROC diagram, furthermore, demonstrates how a random classifier would perform against our model. A random classifier would have a linear line going from the origin to (1,1) (representing a completely linear relaitonship between false and true positive rates). On the other hand, our model is clearly much better at classifying more accurately, with an AUC (Area Under the ROC Curve) of 0.83. This value signifies that our model is good at discriminating between positive and negative classes. Our second graph demonstrates the model’s relationship between precision and recall. Note that precision is defined as the number of True Positives over the number of True Positives + False Positives (i.e. when it predictes positive, how often is it correct), and recall is defines as the number of True Positives over the number of True Positives + False Negatives. 
As one can see, there is a clear tradeoff and our ideal model exists around the 0.6 recall with 0.8 precision.</p>

<h3 id="gaussian-mixture-model">Gaussian Mixture Model</h3>
<p>We also used a GMM, where we intitialized the initial cluster means to the means of each label to incentivize the GMM clusters to be labeled correctly. We achieved a much greater accuracy on our test set, only mislabeling 15 images, which we have displayed below. Our main trouble came from classifying N vs M, and U vs R, which are both pairs of incredibly similar letters in the ASL alphabet.</p>

<p style="font-family:'Courier New'">Accuracy: 0.905 <br /> Precision: 0.914 <br /> Recall: 0.905 <br /> F1 Score:  0.901</p>

<p><img src="assets/img/wrong_predictions.png" width="800" /></p>

<p>To analyze this phenomenon, we used PCA on our dataset and projected the data onto the first 3 principle components so that we can visualize the data. Below, we have the results of this analysis.</p>

<p><img src="assets/img/pca_results.jpg" width="800" /></p>

<p>This makes it clear that visually similar letters (like U and R) are in an incredibly complicated relationship and will be harder to classify than visually different letters (like A and B).</p>

<p>We had better results with this model but may need more testing to see what we can improve. As a whole, we were satisfied with our results, though we hope to potentially integrate more advanced and accurate predictive models (like a CNN) to better predict our ASL alphabet through images. We would also like to preprocess and integrate many more datasets to make our model even more robust.</p>

<h2 id="timeline">Timeline</h2>
<p><img src="assets/img/timeline.png?raw=true" alt="Project Timeline" title="Timeline" /></p>

<h2 id="contributions">Contributions</h2>

<table>
  <tr>
    <th></th>
    <th>Eric Zhou</th>
    <th>Ishan Arora</th>
    <th>Nick Papciak</th>
    <th>Rishi Machanpalli</th>
    <th>Tom Jeong</th>
  </tr>
  <tr>
    <th><b>Proposal</b></th>
    <td>project script and slides</td>
    <td>project script and slides</td>
    <td>GitHub pages and project proposal</td>
    <td>project proposal presentation</td>
    <td>timeline and project proposal</td>
  </tr>
  <tr>
    <th><b>Midterm</b></th>
    <td>MediaPipe processing and midterm report</td>
    <td>KNN training and results / analysis</td>
    <td>GMM and PCA analysis</td>
    <td>MediaPipe processing and midterm report</td>
    <td>KNN training and midterm report</td>
  </tr>
</table>

<h2 id="references">References</h2>
<p>Okan Köpüklü, Ahmet Gunduz, Neslihan Kose, &amp; Gerhard Rigoll. (2019). Real-time Hand Gesture Detection and Classification Using Convolutional Neural Networks.</p>

<p>Metaxas, D., Dilsizian, M., &amp; Neidle, C. (2018). Scalable ASL Sign Recognition using Model-based Machine Learning and Linguistically Annotated Corpora. In Proceedings of the LREC2018 8th Workshop on the Representation and Processing of Sign Languages: Involving the Language Community (pp. 127–132). European Language Resources Association (ELRA).</p>

<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, &amp; Illia Polosukhin. (2023). Attention Is All You Need.</p>

<p>Boulay, Bernard et al. “Human Posture Recognition in Video Sequence.” (2003).</p>

<p>Hand image from the <a href="https://developers.google.com/mediapipe/solutions/vision/hand_landmarker">MediaPipe documentation</a></p>

<p>Sidebar image by vectorjuice <a href="https://www.freepik.com/free-vector/sign-language-classes-abstract-concept-vector-illustration-study-sign-language-translation-voiceless-basic-communication-silent-speech-online-classes-learn-gesture-alphabet-abstract-metaphor_12469769.htm#query=voiceless&amp;position=0&amp;from_view=keyword&amp;track=sph">on Freepik</a></p>



      </section>
      <footer>
        
        <p>Developed by Eric Zhou, Ishan Arora, Nick Papciak, Rishi Machanpalli, and Tom Jeong</a></p>
        
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
